{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9897338,"sourceType":"datasetVersion","datasetId":6079423},{"sourceId":9897391,"sourceType":"datasetVersion","datasetId":6079453},{"sourceId":9907446,"sourceType":"datasetVersion","datasetId":6086990}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, GemmaConfig, AutoTokenizer, AutoModel, MistralConfig, MistralModel, MistralForCausalLM, LlamaConfig, LlamaForCausalLM\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport json\nimport pickle\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:00:00.218064Z","iopub.execute_input":"2024-11-14T16:00:00.218420Z","iopub.status.idle":"2024-11-14T16:00:10.005721Z","shell.execute_reply.started":"2024-11-14T16:00:00.218388Z","shell.execute_reply":"2024-11-14T16:00:10.004891Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/russian-data/merged.csv')\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:00:22.123371Z","iopub.execute_input":"2024-11-14T16:00:22.124304Z","iopub.status.idle":"2024-11-14T16:01:04.061166Z","shell.execute_reply.started":"2024-11-14T16:00:22.124267Z","shell.execute_reply":"2024-11-14T16:01:04.060420Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                 filename                                            content\n0       !Action Pact!.txt  !Action Pact! — лондонская рок-группа, образов...\n1      ( ) (значения).txt  ( ) может означать:\\n\\nСкобки\\n( ) — третий ст...\n2      (1001) Гауссия.txt  (1001) Гауссия (нем. Gaussia) — довольно крупн...\n3  (10266) Владишухов.txt  (10266) Владишухов (лат. Vladishukhov) — типич...\n4      (105) Артемида.txt  (105) Артеми́да (лат. Artemis) — астероид из г...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>!Action Pact!.txt</td>\n      <td>!Action Pact! — лондонская рок-группа, образов...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>( ) (значения).txt</td>\n      <td>( ) может означать:\\n\\nСкобки\\n( ) — третий ст...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(1001) Гауссия.txt</td>\n      <td>(1001) Гауссия (нем. Gaussia) — довольно крупн...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(10266) Владишухов.txt</td>\n      <td>(10266) Владишухов (лат. Vladishukhov) — типич...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(105) Артемида.txt</td>\n      <td>(105) Артеми́да (лат. Artemis) — астероид из г...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"data = data.drop(columns = ['filename'])\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:01:04.062625Z","iopub.execute_input":"2024-11-14T16:01:04.062962Z","iopub.status.idle":"2024-11-14T16:01:04.070505Z","shell.execute_reply.started":"2024-11-14T16:01:04.062929Z","shell.execute_reply":"2024-11-14T16:01:04.069863Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                             content\n0  !Action Pact! — лондонская рок-группа, образов...\n1  ( ) может означать:\\n\\nСкобки\\n( ) — третий ст...\n2  (1001) Гауссия (нем. Gaussia) — довольно крупн...\n3  (10266) Владишухов (лат. Vladishukhov) — типич...\n4  (105) Артеми́да (лат. Artemis) — астероид из г...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>!Action Pact! — лондонская рок-группа, образов...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>( ) может означать:\\n\\nСкобки\\n( ) — третий ст...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(1001) Гауссия (нем. Gaussia) — довольно крупн...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(10266) Владишухов (лат. Vladishukhov) — типич...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(105) Артеми́да (лат. Artemis) — астероид из г...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/tokenizer-6/t6\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:01:04.071271Z","iopub.execute_input":"2024-11-14T16:01:04.071507Z","iopub.status.idle":"2024-11-14T16:01:04.255891Z","shell.execute_reply.started":"2024-11-14T16:01:04.071483Z","shell.execute_reply":"2024-11-14T16:01:04.255143Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"len(tokenizer.vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:01:04.257400Z","iopub.execute_input":"2024-11-14T16:01:04.257664Z","iopub.status.idle":"2024-11-14T16:01:04.280786Z","shell.execute_reply.started":"2024-11-14T16:01:04.257638Z","shell.execute_reply":"2024-11-14T16:01:04.280114Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"32001"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"config = LlamaConfig(\n    vocab_size=len(tokenizer.vocab),       # Keep this standard or adjust for your use case\n    hidden_size=256,        # Reduce from the standard size (e.g., 2048 or 1024)\n    num_hidden_layers=6,   # Reduce the number of transformer layers\n    num_attention_heads=8,  # Reduce the number of attention heads\n    intermediate_size=512, # Adjust feed-forward layer size (standard is usually 4x hidden size)\n    max_position_embeddings=256,  # Adjust as per use case\n    hidden_dropout_prob=0.1,\n    attention_probs_dropout_prob=0.1,\n)\n\n# Create the model with the adjusted configuration\nmodel = LlamaForCausalLM(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:01:04.281663Z","iopub.execute_input":"2024-11-14T16:01:04.281919Z","iopub.status.idle":"2024-11-14T16:01:04.636152Z","shell.execute_reply.started":"2024-11-14T16:01:04.281890Z","shell.execute_reply":"2024-11-14T16:01:04.635400Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"for i,j in model.named_parameters():\n\n  if j.requires_grad and len(j.size()) > 1:\n\n    init.xavier_uniform_(j.data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:01:04.637266Z","iopub.execute_input":"2024-11-14T16:01:04.637525Z","iopub.status.idle":"2024-11-14T16:01:04.765094Z","shell.execute_reply.started":"2024-11-14T16:01:04.637500Z","shell.execute_reply":"2024-11-14T16:01:04.764365Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"total_param=0\n\nfor i,j in model.named_parameters():\n\n    total_param += j.numel()\n\nprint(total_param/(10**6))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:01:04.766025Z","iopub.execute_input":"2024-11-14T16:01:04.766271Z","iopub.status.idle":"2024-11-14T16:01:04.770519Z","shell.execute_reply.started":"2024-11-14T16:01:04.766246Z","shell.execute_reply":"2024-11-14T16:01:04.769906Z"}},"outputs":[{"name":"stdout","text":"20.32\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# model.save_pretrained(\"/kaggle/working/russian_model_2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:35:08.234040Z","iopub.execute_input":"2024-11-14T08:35:08.234274Z","iopub.status.idle":"2024-11-14T08:35:08.245093Z","shell.execute_reply.started":"2024-11-14T08:35:08.234250Z","shell.execute_reply":"2024-11-14T08:35:08.244371Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"data[\"content\"] = data[\"content\"].astype(str)  # Ensure all values are strings\n\ndata = data.dropna(subset=[\"content\"])         # Drop rows with NaN in \"content\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:01:06.741512Z","iopub.execute_input":"2024-11-14T16:01:06.742209Z","iopub.status.idle":"2024-11-14T16:01:06.749624Z","shell.execute_reply.started":"2024-11-14T16:01:06.742156Z","shell.execute_reply":"2024-11-14T16:01:06.748986Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"input_ids = tokenizer(data['content'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")[\"input_ids\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:01:07.137521Z","iopub.execute_input":"2024-11-14T16:01:07.138156Z","iopub.status.idle":"2024-11-14T16:05:44.029855Z","shell.execute_reply.started":"2024-11-14T16:01:07.138123Z","shell.execute_reply":"2024-11-14T16:05:44.028848Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# token_list = []\n\n# for i in input_ids:\n\n#     token_list.extend(i)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.040946Z","iopub.execute_input":"2024-11-14T08:39:50.041227Z","iopub.status.idle":"2024-11-14T08:39:50.044717Z","shell.execute_reply.started":"2024-11-14T08:39:50.041199Z","shell.execute_reply":"2024-11-14T08:39:50.043982Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# len(token_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.046216Z","iopub.execute_input":"2024-11-14T08:39:50.046555Z","iopub.status.idle":"2024-11-14T08:39:50.056345Z","shell.execute_reply.started":"2024-11-14T08:39:50.046518Z","shell.execute_reply":"2024-11-14T08:39:50.055679Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# # Convert the token_list to a NumPy array\n# import numpy as np\n# token_array = np.array(token_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.057289Z","iopub.execute_input":"2024-11-14T08:39:50.057576Z","iopub.status.idle":"2024-11-14T08:39:50.067402Z","shell.execute_reply.started":"2024-11-14T08:39:50.057548Z","shell.execute_reply":"2024-11-14T08:39:50.066712Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# context_len = 256  # Batch size\n\n# # Split the token_array into batches of size context_len\n\n# num_batches = len(token_array) // context_len  # Calculate how many full batches there are\n\n# token_batches = np.array_split(token_array[:num_batches * context_len], num_batches)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.068318Z","iopub.execute_input":"2024-11-14T08:39:50.068597Z","iopub.status.idle":"2024-11-14T08:39:50.079427Z","shell.execute_reply.started":"2024-11-14T08:39:50.068570Z","shell.execute_reply":"2024-11-14T08:39:50.078771Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ids = pd.DataFrame(columns=[\"input_ids\"])\n# ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.080256Z","iopub.execute_input":"2024-11-14T08:39:50.080512Z","iopub.status.idle":"2024-11-14T08:39:50.092754Z","shell.execute_reply.started":"2024-11-14T08:39:50.080486Z","shell.execute_reply":"2024-11-14T08:39:50.092068Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ids[\"input_ids\"] = token_batches\n# ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.093580Z","iopub.execute_input":"2024-11-14T08:39:50.093832Z","iopub.status.idle":"2024-11-14T08:39:50.101741Z","shell.execute_reply.started":"2024-11-14T08:39:50.093807Z","shell.execute_reply":"2024-11-14T08:39:50.101052Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# from datasets import Dataset,DatasetDict\n# from datasets import load_dataset\n# import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.102610Z","iopub.execute_input":"2024-11-14T08:39:50.102860Z","iopub.status.idle":"2024-11-14T08:39:50.111171Z","shell.execute_reply.started":"2024-11-14T08:39:50.102834Z","shell.execute_reply":"2024-11-14T08:39:50.110478Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# hf_dataset=Dataset.from_pandas(ids)\n\n# hf_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.112023Z","iopub.execute_input":"2024-11-14T08:39:50.112261Z","iopub.status.idle":"2024-11-14T08:39:50.120822Z","shell.execute_reply.started":"2024-11-14T08:39:50.112236Z","shell.execute_reply":"2024-11-14T08:39:50.120073Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# spilt_dataset=hf_dataset.train_test_split(test_size=0.1)\n\n# train_dataset=spilt_dataset['train']\n\n# eval_dataset=spilt_dataset['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.121954Z","iopub.execute_input":"2024-11-14T08:39:50.122285Z","iopub.status.idle":"2024-11-14T08:39:50.138590Z","shell.execute_reply.started":"2024-11-14T08:39:50.122257Z","shell.execute_reply":"2024-11-14T08:39:50.137872Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# train_dataset.to_parquet(\"russ_dataset_token_train.parquet\")\n\n# eval_dataset.to_parquet(\"russ_dataset_token_test.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.139505Z","iopub.execute_input":"2024-11-14T08:39:50.139767Z","iopub.status.idle":"2024-11-14T08:39:50.148727Z","shell.execute_reply.started":"2024-11-14T08:39:50.139741Z","shell.execute_reply":"2024-11-14T08:39:50.148086Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# from transformers import TrainingArguments, Trainer\n\n# import math\n\n\n\n# # # Load model and tokenizer\n\n# # model = LlamaForCausalLM.from_pretrained(\"path/to/your/model\")\n\n# # tokenizer = AutoTokenizer.from_pretrained(\"path/to/your/model\")\n\n\n\n# # Set training arguments\n\n# training_args = TrainingArguments(\n\n#     output_dir=\"/kaggle/working/russian_model/checkpoints\",  # Directory to save checkpoints\n\n#     per_device_train_batch_size=4,\n\n#     per_device_eval_batch_size=4,\n\n#     num_train_epochs=10,  # Adjust as needed\n\n#     save_steps=500,  # Save model every 500 steps or adjust as needed\n\n#     logging_dir=\"/kaggle/working/t6/russian_model/logs\",  # Directory for logs\n\n#     logging_steps=50,  # Log metrics every 50 steps\n\n#     evaluation_strategy=\"steps\",  # Evaluate at specific intervals\n\n#     eval_steps=50,\n\n#     save_total_limit=2,  # Keep only the last 2 checkpoints\n\n#     load_best_model_at_end=True,  # Load the best model at the end of training\n\n# )\n\n\n\n\n\n\n\n# # # Train the model\n\n# # trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.149562Z","iopub.execute_input":"2024-11-14T08:39:50.149793Z","iopub.status.idle":"2024-11-14T08:39:50.160217Z","shell.execute_reply.started":"2024-11-14T08:39:50.149769Z","shell.execute_reply":"2024-11-14T08:39:50.159549Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# import math\n# import numpy as np\n# import pandas as pd\n# import torch\n\n# # Modify the Trainer initialization\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=eval_dataset,\n#     tokenizer=tokenizer,\n#     compute_metrics=lambda eval_pred: {\n#         \"eval_loss\": eval_pred.loss if hasattr(eval_pred, \"loss\") else None\n#     }\n# )\n\n\n# # Custom training loop to track perplexity at every 0.1 epoch\n# steps_per_epoch = len(train_dataset) // training_args.per_device_train_batch_size\n# checkpoint_steps = steps_per_epoch // 10  # Steps for 0.1 epoch logging\n\n# perplexities = []\n\n# for epoch in range(int(training_args.num_train_epochs * 10)):\n#     # Trainer training logic (if partial training per 0.1 epoch is needed)\n#     eval_results = trainer.evaluate()\n\n#     if \"eval_loss\" not in eval_results or eval_results[\"eval_loss\"] is None:\n#         inputs = eval_dataset[:training_args.per_device_eval_batch_size][\"input_ids\"]\n#         labels = inputs.clone()  # Adjust to match your dataset\n\n#         with torch.no_grad():\n#             outputs = model(input_ids=inputs, labels=labels)\n#             loss = outputs.loss\n\n#         eval_results[\"eval_loss\"] = loss.item()\n\n#     perplexity = math.exp(eval_results[\"eval_loss\"])\n#     perplexities.append(perplexity)\n#     print(f\"Epoch {epoch / 10:.1f}: Perplexity = {perplexity}\")\n\n# # Displaying perplexity as a matrix\n# epochs = np.arange(0, training_args.num_train_epochs, 0.1)\n# df_perplexity = pd.DataFrame({\"Epoch\": epochs, \"Perplexity\": perplexities})\n# print(df_perplexity)\n\n# # Optionally, display as a table with epochs as index\n# df_perplexity.set_index(\"Epoch\", inplace=True)\n# display(df_perplexity)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.161110Z","iopub.execute_input":"2024-11-14T08:39:50.161379Z","iopub.status.idle":"2024-11-14T08:39:50.171433Z","shell.execute_reply.started":"2024-11-14T08:39:50.161351Z","shell.execute_reply":"2024-11-14T08:39:50.170751Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# # Tokenization\n\n# # Define training parameters\n# batch_size = 32\n# learning_rate = 3e-5\n# epochs = 10\n# perplexity_values = []\n\n# # Setup DataLoader for batching\n# train_data = torch.utils.data.TensorDataset(input_ids)\n# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\n# # Optimizer and loss\n# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n# criterion = nn.CrossEntropyLoss()\n\n# # Training loop with perplexity tracking and checkpoint saving\n# for epoch in range(epochs):\n#     model.train()\n#     epoch_loss = 0\n#     num_batches = len(train_loader)\n\n#     for batch_num, batch in enumerate(train_loader):\n#         inputs = batch[0].to(model.device)\n\n#         # Forward pass\n#         outputs = model(inputs, labels=inputs)\n#         loss = outputs.loss\n#         epoch_loss += loss.item()\n\n#         # Backward pass and optimization\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         # Record perplexity at every 0.1 epoch and save checkpoint\n#         if batch_num % (num_batches // 10) == 0:\n#             perplexity = torch.exp(torch.tensor(epoch_loss / (batch_num + 1)))\n#             perplexity_values.append(perplexity.item())\n#             print(f\"Perplexity at {epoch + (batch_num / num_batches):.1f} epoch: {perplexity.item()}\")\n\n#             # Save model checkpoint\n#             checkpoint_path = f\"./model_checkpoint_epoch_{epoch + 1}batch{batch_num + 1}.pth\"\n#             torch.save({\n#                 'epoch': epoch + 1,\n#                 'batch_num': batch_num + 1,\n#                 'model_state_dict': model.state_dict(),\n#                 'optimizer_state_dict': optimizer.state_dict(),\n#                 'loss': loss.item()\n#             }, checkpoint_path)\n#             print(f\"Checkpoint saved at {checkpoint_path}\")\n\n#     # Save model at the end of each epoch\n#     # model_save_path = f\"./model_epoch_{epoch + 1}.pth\"\n#     # torch.save(model.state_dict(), model_save_path)\n#     # print(f\"Model saved at {model_save_path}\")\n\n# # Convert perplexity values into a DataFrame for easier visualization\n# df_perplexity = pd.DataFrame(perplexity_values, columns=['Perplexity'])\n# print(df_perplexity)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.172553Z","iopub.execute_input":"2024-11-14T08:39:50.172806Z","iopub.status.idle":"2024-11-14T08:39:50.200606Z","shell.execute_reply.started":"2024-11-14T08:39:50.172779Z","shell.execute_reply":"2024-11-14T08:39:50.199734Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\n\n# Define training parameters\nbatch_size = 32\nlearning_rate = 3e-5\nepochs = 5\nperplexity_values = []\n\n# Setup DataLoader for batching\ntrain_data = torch.utils.data.TensorDataset(input_ids)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\n# Optimizer and loss\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop with perplexity tracking and checkpoint saving\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss = 0\n    num_batches = len(train_loader)\n\n    for batch_num, batch in enumerate(train_loader):\n        inputs = batch[0].to(model.device)\n\n        # Forward pass\n        outputs = model(inputs, labels=inputs)\n        loss = outputs.loss\n        epoch_loss += loss.item()\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Record perplexity at every 0.1 epoch\n        if batch_num % (num_batches // 10) == 0:\n            avg_loss = epoch_loss / (batch_num + 1)\n            perplexity = torch.exp(torch.tensor(avg_loss))\n            perplexity_values.append(perplexity.item())\n            print(f\"Perplexity at {epoch + (batch_num / num_batches):.1f} epoch: {perplexity.item()}\")\n\n    # Save model checkpoint at the end of each epoch\n    checkpoint_path = f\"./model_checkpoint_epoch_{epoch + 1}.pth\"\n    torch.save({\n        'epoch': epoch + 1,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss.item()\n    }, checkpoint_path)\n    print(f\"Checkpoint saved at {checkpoint_path}\")\n\n# Convert perplexity values into a DataFrame for easier visualization\ndf_perplexity = pd.DataFrame(perplexity_values, columns=['Perplexity'])\nprint(df_perplexity)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T08:39:50.204399Z","iopub.execute_input":"2024-11-14T08:39:50.204663Z"}},"outputs":[{"name":"stdout","text":"Perplexity at 0.0 epoch: 31460.984375\nPerplexity at 0.1 epoch: 25488.380859375\nPerplexity at 0.2 epoch: 22216.86328125\nPerplexity at 0.3 epoch: 19621.55859375\nPerplexity at 0.4 epoch: 17417.72265625\nPerplexity at 0.5 epoch: 15566.9814453125\nPerplexity at 0.6 epoch: 13973.6484375\nPerplexity at 0.7 epoch: 12657.2060546875\nPerplexity at 0.8 epoch: 11539.3310546875\nPerplexity at 0.9 epoch: 10529.8525390625\nPerplexity at 1.0 epoch: 9656.388671875\nCheckpoint saved at ./model_checkpoint_epoch_1.pth\nPerplexity at 1.0 epoch: 4211.74169921875\nPerplexity at 1.1 epoch: 3788.209716796875\nPerplexity at 1.2 epoch: 3688.85302734375\nPerplexity at 1.3 epoch: 3490.93408203125\nPerplexity at 1.4 epoch: 3302.569580078125\nPerplexity at 1.5 epoch: 3127.624267578125\nPerplexity at 1.6 epoch: 2975.689208984375\nPerplexity at 1.7 epoch: 2828.321533203125\nPerplexity at 1.8 epoch: 2712.60546875\nPerplexity at 1.9 epoch: 2608.5791015625\nPerplexity at 2.0 epoch: 2502.0908203125\nCheckpoint saved at ./model_checkpoint_epoch_2.pth\nPerplexity at 2.0 epoch: 1698.34716796875\nPerplexity at 2.1 epoch: 1670.8978271484375\nPerplexity at 2.2 epoch: 1586.85400390625\nPerplexity at 2.3 epoch: 1554.2860107421875\nPerplexity at 2.4 epoch: 1504.78564453125\nPerplexity at 2.5 epoch: 1436.7738037109375\nPerplexity at 2.6 epoch: 1380.107421875\nPerplexity at 2.7 epoch: 1348.5455322265625\nPerplexity at 2.8 epoch: 1312.7540283203125\nPerplexity at 2.9 epoch: 1266.1104736328125\nPerplexity at 3.0 epoch: 1236.5340576171875\nCheckpoint saved at ./model_checkpoint_epoch_3.pth\nPerplexity at 3.0 epoch: 855.7868041992188\nPerplexity at 3.1 epoch: 1025.4052734375\nPerplexity at 3.2 epoch: 945.1337890625\nPerplexity at 3.3 epoch: 863.8785400390625\nPerplexity at 3.4 epoch: 851.1847534179688\nPerplexity at 3.5 epoch: 830.4733276367188\nPerplexity at 3.6 epoch: 808.0430908203125\nPerplexity at 3.7 epoch: 784.3067016601562\nPerplexity at 3.8 epoch: 774.7823486328125\nPerplexity at 3.9 epoch: 745.9658203125\nPerplexity at 4.0 epoch: 730.274169921875\nCheckpoint saved at ./model_checkpoint_epoch_4.pth\nPerplexity at 4.0 epoch: 866.6102294921875\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:59:26.931255Z","iopub.execute_input":"2024-11-14T15:59:26.931604Z","iopub.status.idle":"2024-11-14T15:59:46.404515Z","shell.execute_reply.started":"2024-11-14T15:59:26.931576Z","shell.execute_reply":"2024-11-14T15:59:46.403804Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/input/checkpoint-4/model_checkpoint_epoch_4.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:06:39.108987Z","iopub.execute_input":"2024-11-14T16:06:39.109372Z","iopub.status.idle":"2024-11-14T16:06:41.236017Z","shell.execute_reply.started":"2024-11-14T16:06:39.109341Z","shell.execute_reply":"2024-11-14T16:06:41.235204Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_13/2084999760.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(\"/kaggle/input/checkpoint-4/model_checkpoint_epoch_4.pth\")\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"model.load_state_dict(checkpoint['model_state_dict'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:06:43.236824Z","iopub.execute_input":"2024-11-14T16:06:43.237370Z","iopub.status.idle":"2024-11-14T16:06:43.256158Z","shell.execute_reply.started":"2024-11-14T16:06:43.237320Z","shell.execute_reply":"2024-11-14T16:06:43.255118Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define or import your model architecture\n# model = YourModel()\n\n# Define training parameters\nbatch_size = 32\nlearning_rate = 3e-5\nepochs = 1\nperplexity_values = []\n\n# Setup DataLoader for batching\ntrain_data = torch.utils.data.TensorDataset(input_ids)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n# Load checkpoint\n# checkpoint = torch.load('path/to/checkpoint.pth')\n# model.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\n\n# Training loop with perplexity tracking and checkpoint saving\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss = 0\n    num_batches = len(train_loader)\n\n    for batch_num, batch in enumerate(train_loader):\n        inputs = batch[0].to(model.device)\n\n        # Forward pass\n        outputs = model(inputs, labels=inputs)\n        loss = outputs.loss\n        epoch_loss += loss.item()\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Record perplexity at every 0.1 epoch\n        if batch_num % (num_batches // 10) == 0:\n            avg_loss = epoch_loss / (batch_num + 1)\n            perplexity = torch.exp(torch.tensor(avg_loss))\n            perplexity_values.append(perplexity.item())\n            print(f\"Perplexity at {epoch + (batch_num / num_batches):.1f} epoch: {perplexity.item()}\")\n\n    # Save model checkpoint at the end of each epoch\n    checkpoint_path = f\"./model_checkpoint_epoch_{epoch + 1}.pth\"\n    torch.save({\n        'epoch': epoch + 1,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss.item()\n    }, checkpoint_path)\n    print(f\"Checkpoint saved at {checkpoint_path}\")\n\n# Convert perplexity values into a DataFrame for easier visualization\ndf_perplexity = pd.DataFrame(perplexity_values, columns=['Perplexity'])\nprint(df_perplexity)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:13:39.072180Z","iopub.execute_input":"2024-11-14T16:13:39.072599Z","iopub.status.idle":"2024-11-14T16:39:14.232354Z","shell.execute_reply.started":"2024-11-14T16:13:39.072485Z","shell.execute_reply":"2024-11-14T16:39:14.231519Z"}},"outputs":[{"name":"stdout","text":"Perplexity at 0.0 epoch: 490.95758056640625\nPerplexity at 0.1 epoch: 599.7506103515625\nPerplexity at 0.2 epoch: 627.1124267578125\nPerplexity at 0.3 epoch: 652.8843383789062\nPerplexity at 0.4 epoch: 677.440673828125\nPerplexity at 0.5 epoch: 682.7405395507812\nPerplexity at 0.6 epoch: 695.3709716796875\nPerplexity at 0.7 epoch: 701.662841796875\nPerplexity at 0.8 epoch: 700.6688232421875\nPerplexity at 0.9 epoch: 703.2313842773438\nPerplexity at 1.0 epoch: 706.0648803710938\nCheckpoint saved at ./model_checkpoint_epoch_1.pth\n    Perplexity\n0   490.957581\n1   599.750610\n2   627.112427\n3   652.884338\n4   677.440674\n5   682.740540\n6   695.370972\n7   701.662842\n8   700.668823\n9   703.231384\n10  706.064880\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"torch.save(model.state_dict(), \"Token_titans.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:42:41.720440Z","iopub.execute_input":"2024-11-14T16:42:41.720825Z","iopub.status.idle":"2024-11-14T16:42:41.840114Z","shell.execute_reply.started":"2024-11-14T16:42:41.720790Z","shell.execute_reply":"2024-11-14T16:42:41.839045Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:43:26.370827Z","iopub.execute_input":"2024-11-14T16:43:26.371468Z","iopub.status.idle":"2024-11-14T16:43:26.374807Z","shell.execute_reply.started":"2024-11-14T16:43:26.371434Z","shell.execute_reply":"2024-11-14T16:43:26.374062Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Define the Russian prompt text\nprompt_text = ['Расскажи мне о самых известных русских писателях.',\n                'Опиши события Октябрьской революции.',\n                'Какие популярные русские народные сказки ты знаешь',\n                'Что ты можешь рассказать о культуре и традициях России',\n                'Как развивалась русская живопись в веке',\n                'Назови самые посещаемые туристические места в Москве',\n                'Объясни, как функционирует российская политическая система',\n                'Какие научные достижения сделали российские ученые',\n                'Как празднуют Новый год в России',\n                'Что такое балет и как он связан с Россией']\n\nfor x in prompt_text:\n    # Tokenize the prompt with attention mask\n    inputs = tokenizer(x, return_tensors=\"pt\", padding=True)\n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n\n    # Ensure model is in evaluation mode\n    model.eval()\n\n    # Define a function to calculate perplexity and print output text\n    def calculate_perplexity_and_output(input_ids, attention_mask, model):\n        # Get model predictions and loss with attention mask\n        with torch.no_grad():\n            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n            loss = outputs.loss  # Cross-entropy loss\n\n        # Calculate perplexity\n        perplexity = math.exp(loss.item())\n\n        # Generate model output text with pad_token_id set to eos_token_id\n        output_ids = output_ids = model.generate(\n        input_ids,\n        attention_mask=attention_mask,\n        max_length=50,\n        num_return_sequences=1,\n        pad_token_id=tokenizer.eos_token_id,\n        do_sample=True,\n        temperature=0.7\n    )\n\n\n        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n        return perplexity, output_text\n\n    # Calculate perplexity and generate output\n    perplexity, output_text = calculate_perplexity_and_output(input_ids, attention_mask, model)\n    print(f\"Perplexity: {perplexity}\")\n    print(f\"Output Text: {output_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:43:28.921643Z","iopub.execute_input":"2024-11-14T16:43:28.921999Z","iopub.status.idle":"2024-11-14T16:43:48.795442Z","shell.execute_reply.started":"2024-11-14T16:43:28.921969Z","shell.execute_reply":"2024-11-14T16:43:48.794504Z"}},"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"Perplexity: 11144.589065281632\nOutput Text: Расскажи мне о самых известных русских писателях.==В  \nPerplexity: 13449.416201619513\nOutput Text: Опиши события Октябрьской революции. В  \nPerplexity: 15101.308266880347\nOutput Text: Какие популярные русские народные сказки ты знаешь в  - и и  , года по \nPerplexity: 11564.636794313808\nOutput Text: Что ты можешь рассказать о культуре и традициях России ита с по у было — , годуных из и   \nPerplexity: 9671.466155425449\nOutput Text: Как развивалась русская живопись в веке и -- ,\nPerplexity: 11593.977731867488\nOutput Text: Назови самые посещаемые туристические места в Москве в \nPerplexity: 19461.009908695858\nOutput Text: Объясни, как функционирует российская политическая система) \nPerplexity: 14139.227926541418\nOutput Text: Какие научные достижения сделали российские ученые и из  - \nPerplexity: 5502.527013917124\nOutput Text: Как празднуют Новый год в Россиим в  ==С \nPerplexity: 3197.7808511367366\nOutput Text: Что такое балет и как он связан с Россией::\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}