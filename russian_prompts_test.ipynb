{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9899605,"sourceType":"datasetVersion","datasetId":6081055},{"sourceId":9899621,"sourceType":"datasetVersion","datasetId":6081066},{"sourceId":9907532,"sourceType":"datasetVersion","datasetId":6087058}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, GemmaConfig, AutoTokenizer, AutoModel, MistralConfig, MistralModel, MistralForCausalLM, LlamaConfig, LlamaForCausalLM\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport json\nimport pickle\nimport pandas as pd","metadata":{"_uuid":"0749204f-4ae8-449e-b5e6-e7639e110c99","_cell_guid":"a59dcbb4-fe7f-40d9-a417-ee06fbf91baa","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-14T16:22:40.359514Z","iopub.execute_input":"2024-11-14T16:22:40.359900Z","iopub.status.idle":"2024-11-14T16:22:40.364746Z","shell.execute_reply.started":"2024-11-14T16:22:40.359867Z","shell.execute_reply":"2024-11-14T16:22:40.363918Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"input_ids = torch.load('/kaggle/input/tensor/tensor.pt')","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:22:41.260527Z","iopub.execute_input":"2024-11-14T16:22:41.261167Z","iopub.status.idle":"2024-11-14T16:22:41.699852Z","shell.execute_reply.started":"2024-11-14T16:22:41.261131Z","shell.execute_reply":"2024-11-14T16:22:41.698867Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_13/1273661524.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  input_ids = torch.load('/kaggle/input/tensor/tensor.pt')\n","output_type":"stream"}]},{"cell_type":"code","source":"input_ids","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:22:42.220477Z","iopub.execute_input":"2024-11-14T16:22:42.221373Z","iopub.status.idle":"2024-11-14T16:22:42.227477Z","shell.execute_reply.started":"2024-11-14T16:22:42.221342Z","shell.execute_reply":"2024-11-14T16:22:42.226712Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"tensor([[ 2653,    42,  9355,  ...,  4202,  4391, 15817],\n        [    0,     0,     0,  ...,     6,     9,  6885],\n        [    0,     0,     0,  ...,     7,  2557,  6060],\n        ...,\n        [    0,     0,     0,  ..., 20936,  1081,  5880],\n        [    0,     0,     0,  ..., 24459, 18840,  6060],\n        [    0,     0,     0,  ..., 24459, 18840,  6060]])"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/tokenize\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:22:43.175313Z","iopub.execute_input":"2024-11-14T16:22:43.175947Z","iopub.status.idle":"2024-11-14T16:22:43.243957Z","shell.execute_reply.started":"2024-11-14T16:22:43.175912Z","shell.execute_reply":"2024-11-14T16:22:43.243107Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"config = LlamaConfig(\n    vocab_size=len(tokenizer.vocab),       # Keep this standard or adjust for your use case\n    hidden_size=256,        # Reduce from the standard size (e.g., 2048 or 1024)\n    num_hidden_layers=6,   # Reduce the number of transformer layers\n    num_attention_heads=8,  # Reduce the number of attention heads\n    intermediate_size=512, # Adjust feed-forward layer size (standard is usually 4x hidden size)\n    max_position_embeddings=256,  # Adjust as per use case\n    hidden_dropout_prob=0.1,\n    attention_probs_dropout_prob=0.1,\n)\n\n# Create the model with the adjusted configuration\nmodel = LlamaForCausalLM(config)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:22:44.512851Z","iopub.execute_input":"2024-11-14T16:22:44.513152Z","iopub.status.idle":"2024-11-14T16:22:44.820195Z","shell.execute_reply.started":"2024-11-14T16:22:44.513126Z","shell.execute_reply":"2024-11-14T16:22:44.819230Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Load the model's state_dict from the file path\nmodel_path = \"/kaggle/input/model-77/model_epoch_2 (7).pth\"\nmodel.load_state_dict(torch.load(model_path))","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:22:46.880279Z","iopub.execute_input":"2024-11-14T16:22:46.881010Z","iopub.status.idle":"2024-11-14T16:22:46.961445Z","shell.execute_reply.started":"2024-11-14T16:22:46.880964Z","shell.execute_reply":"2024-11-14T16:22:46.960708Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_13/4244569867.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"import math","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:22:48.530999Z","iopub.execute_input":"2024-11-14T16:22:48.531525Z","iopub.status.idle":"2024-11-14T16:22:48.535034Z","shell.execute_reply.started":"2024-11-14T16:22:48.531492Z","shell.execute_reply":"2024-11-14T16:22:48.534359Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Define the Russian prompt text\nprompt_text = \"Введите текст на русском языке здесь.\"\n\n# Tokenize the prompt with attention mask\ninputs = tokenizer(prompt_text, return_tensors=\"pt\", padding=True)\ninput_ids = inputs['input_ids']\nattention_mask = inputs['attention_mask']\n\n# Ensure model is in evaluation mode\nmodel.eval()\n\n# Define a function to calculate perplexity and print output text\ndef calculate_perplexity_and_output(input_ids, attention_mask, model):\n    # Get model predictions and loss with attention mask\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n        loss = outputs.loss  # Cross-entropy loss\n\n    # Calculate perplexity\n    perplexity = math.exp(loss.item())\n\n    # Generate model output text with pad_token_id set to eos_token_id\n    output_ids = output_ids = model.generate(\n    input_ids,\n    attention_mask=attention_mask,\n    max_length=50,\n    num_return_sequences=1,\n    pad_token_id=tokenizer.eos_token_id,\n    do_sample=True,\n    temperature=0.7\n)\n\n\n    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n    return perplexity, output_text\n\n# Calculate perplexity and generate output\nperplexity, output_text = calculate_perplexity_and_output(input_ids, attention_mask, model)\nprint(f\"Perplexity: {perplexity}\")\nprint(f\"Output Text: {output_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:23:33.514347Z","iopub.execute_input":"2024-11-14T16:23:33.514759Z","iopub.status.idle":"2024-11-14T16:23:35.586615Z","shell.execute_reply.started":"2024-11-14T16:23:33.514716Z","shell.execute_reply":"2024-11-14T16:23:35.585722Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"Perplexity: 574.050516104102\nOutput Text: Введите текст на русском языке здесь. В русском языке слово обозначены это только и в русском языке в русском языке — это книга даминана, который обозначают ацен-советы и его как правило — слово и слово «со\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the Russian prompt text\nprompt_text = ['Расскажи мне о самых известных русских писателях.',\n                'Опиши события Октябрьской революции.',\n                'Какие популярные русские народные сказки ты знаешь',\n                'Что ты можешь рассказать о культуре и традициях России',\n                'Как развивалась русская живопись в веке',\n                'Назови самые посещаемые туристические места в Москве',\n                'Объясни, как функционирует российская политическая система',\n                'Какие научные достижения сделали российские ученые',\n                'Как празднуют Новый год в России',\n                'Что такое балет и как он связан с Россией']\n\nfor x in prompt_text:\n    # Tokenize the prompt with attention mask\n    inputs = tokenizer(x, return_tensors=\"pt\", padding=True)\n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n\n    # Ensure model is in evaluation mode\n    model.eval()\n\n    # Define a function to calculate perplexity and print output text\n    def calculate_perplexity_and_output(input_ids, attention_mask, model):\n        # Get model predictions and loss with attention mask\n        with torch.no_grad():\n            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n            loss = outputs.loss  # Cross-entropy loss\n\n        # Calculate perplexity\n        perplexity = math.exp(loss.item())\n\n        # Generate model output text with pad_token_id set to eos_token_id\n        output_ids = output_ids = model.generate(\n        input_ids,\n        attention_mask=attention_mask,\n        max_length=50,\n        num_return_sequences=1,\n        pad_token_id=tokenizer.eos_token_id,\n        do_sample=True,\n        temperature=0.7\n    )\n\n\n        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n        return perplexity, output_text\n\n    # Calculate perplexity and generate output\n    perplexity, output_text = calculate_perplexity_and_output(input_ids, attention_mask, model)\n    print(f\"Perplexity: {perplexity}\")\n    print(f\"Output Text: {output_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:28:38.744310Z","iopub.execute_input":"2024-11-14T16:28:38.744633Z","iopub.status.idle":"2024-11-14T16:28:59.057885Z","shell.execute_reply.started":"2024-11-14T16:28:38.744605Z","shell.execute_reply":"2024-11-14T16:28:59.056739Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Perplexity: 758.797789466119\nOutput Text: Расскажи мне о самых известных русских писателях.В творчестве рассказа о жизни; с восемнадцати лет — некий вечер. Создал на сцене театрального театра; у него не очень много и не могли менять.Внезап\nPerplexity: 175.21097679300226\nOutput Text: Опиши события Октябрьской революции.== Сведения о жизни ==Повесть о жизни:Акарити ИзабелевскаяАкаритри.Акасикотематика\nPerplexity: 477.0364973487989\nOutput Text: Какие популярные русские народные сказки ты знаешь. На лице изображены три чёрные песни, как и тридца, а в каждом разновидности, в которых все и в свете.== Распространение ==Двух\nPerplexity: 212.83465453822186\nOutput Text: Что ты можешь рассказать о культуре и традициях России на Украине в  году, когда ее брат Николай Иванович Ячинский был сослан из села Михайловское уезда в  году, в \nPerplexity: 324.00836979021864\nOutput Text: Как развивалась русская живопись в веке — в виде и мифах, иногда в его коллекциях и искусстве, и в ряде случаев — в искусстве и искусстве.== История ==Круженская картина была написана в\nPerplexity: 490.29410085028815\nOutput Text: Назови самые посещаемые туристические места в Москве в  году и  -е место в истории украинского национального союза национальной федерации федеральных фестивалей.==\nPerplexity: 104.66297143394642\nOutput Text: Объясни, как функционирует российская политическая система== История ==В  году на базе Гвиации в качестве заместителя директора по разработке управления на базе ЦК КПСС с \nPerplexity: 152.49372554486172\nOutput Text: Какие научные достижения сделали российские ученые и геодезисты, а также некоторые научные науки.== История ==В  году завод был создан в  году для производства Института\nPerplexity: 425.77243364827854\nOutput Text: Как празднуют Новый год в России и в  -й день нашей эры.== Направление и героизм, в России ===== Авиационный день ======= Государственный мульт\nPerplexity: 241.06093035325748\nOutput Text: Что такое балет и как он связан с Россией\". Он мог бы сделать с этим влюбленный братом и был его любимого брата и был убит. Так он был очень похож на жизнь своего друга и умер в возрасте  лет.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n# Tokenization\n\n# Define training parameters\nbatch_size = 32\nlearning_rate = 3e-4\nepochs = 1\nperplexity_values = []\n\n# Setup DataLoader for batching\ntrain_data = torch.utils.data.TensorDataset(input_ids)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\n# Optimizer and loss\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop with perplexity tracking and checkpoint saving\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss = 0\n    num_batches = len(train_loader)\n\n    for batch_num, batch in enumerate(train_loader):\n        inputs = batch[0].to(model.device)\n\n        # Forward pass\n        outputs = model(inputs, labels=inputs)\n        loss = outputs.loss\n        epoch_loss += loss.item()\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Record perplexity at every 0.1 epoch and save checkpoint\n        if batch_num % (num_batches // 10) == 0:\n            perplexity = torch.exp(torch.tensor(epoch_loss / (batch_num + 1)))\n            perplexity_values.append(perplexity.item())\n            print(f\"Perplexity at {epoch + (batch_num / num_batches):.1f} epoch: {perplexity.item()}\")\n\n            # Save model checkpoint\n            checkpoint_path = f\"./model_checkpoint_epoch_{epoch + 1 + 2}batch{batch_num + 1+2}.pth\"\n            torch.save({\n                'epoch': epoch + 1,\n                'batch_num': batch_num + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': loss.item()\n            }, checkpoint_path)\n            print(f\"Checkpoint saved at {checkpoint_path}\")\n\n            # Save model at the end of each epoch\n            model_save_path = f\"./model_epoch_{epoch + 2}.pth\"\n            torch.save(model.state_dict(), model_save_path)\n            print(f\"Model saved at {model_save_path}\")\n\n# Convert perplexity values into a DataFrame for easier visualization\ndf_perplexity = pd.DataFrame(perplexity_values, columns=['Perplexity'])\nprint(df_perplexity)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:30:55.044812Z","iopub.execute_input":"2024-11-14T16:30:55.045928Z","iopub.status.idle":"2024-11-14T16:30:55.243800Z","shell.execute_reply.started":"2024-11-14T16:30:55.045886Z","shell.execute_reply":"2024-11-14T16:30:55.242572Z"},"trusted":true},"execution_count":28,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 35\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Record perplexity at every 0.1 epoch and save checkpoint\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"],"ename":"RuntimeError","evalue":"element 0 of tensors does not require grad and does not have a grad_fn","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}